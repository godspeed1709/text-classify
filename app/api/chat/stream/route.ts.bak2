import { auth } from "@clerk/nextjs/server";
import { NextResponse } from "next/server";
import { api } from "@/convex/_generated/api";
import { getConvexClient } from "@/lib/convex";
import {
  ChatRequestBody,
  StreamMessage,
  StreamMessageType,
  SSE_DATA_PREFIX,
  SSE_LINE_DELIMITER,
} from "@/lib/types";

export const runtime = "edge";

function sendSSEMessage(
  writer: WritableStreamDefaultWriter<Uint8Array>,
  data: StreamMessage
) {
  const encoder = new TextEncoder();
  return writer.write(
    encoder.encode(
      `${SSE_DATA_PREFIX}${JSON.stringify(data)}${SSE_LINE_DELIMITER}`
    )
  );
}

// Function to convert numeric class labels to human-readable text
function translateClassLabel(label: string): string {
  // Map of class labels to human-readable names
  const classMap: Record<string, string> = {
    "1": "SPAM",
    "0": "Legitimate",
    // Add more mappings as needed
  };
  
  // Return the mapped value or the original if no mapping exists
  return classMap[label] || label;
}

// Flask API endpoint
const FLASK_API_URL = process.env.FLASK_API_URL || "http://localhost:5000";

export async function POST(req: Request) {
  try {
    const { userId } = await auth();
    if (!userId) {
      return new Response("Unauthorized", { status: 401 });
    }

    const { messages, newMessage, chatId } =
      (await req.json()) as ChatRequestBody;
    const convex = getConvexClient();

    // Create stream with larger queue strategy for better performance
    const stream = new TransformStream({}, { highWaterMark: 1024 });
    const writer = stream.writable.getWriter();

    const response = new Response(stream.readable, {
      headers: {
        "Content-Type": "text/event-stream",
        Connection: "keep-alive",
        "X-Accel-Buffering": "no", // Disable buffering for nginx which is required for SSE to work properly
      },
    });

    // Handle the streaming response
    (async () => {
      try {
        // Send initial connection established message
        await sendSSEMessage(writer, { type: StreamMessageType.Connected });

        // Send user message to Convex
        await convex.mutation(api.messages.send, {
          chatId,
          content: newMessage,
        });

        try {
          // Begin prediction process
          await sendSSEMessage(writer, {
            type: StreamMessageType.ToolStart,
            tool: "text_classification",
            input: newMessage,
          });

          // Make request to Flask API
          const flaskResponse = await fetch(`${FLASK_API_URL}/predict`, {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
            },
            body: JSON.stringify({ text: newMessage }),
          });

          if (!flaskResponse.ok) {
            throw new Error(`Flask API responded with status: ${flaskResponse.status}`);
          }

          // Get prediction result
          const predictionResult = await flaskResponse.json();

          // Translate class name before sending tool end event
          const originalClass = predictionResult.class;
          const translatedClass = translateClassLabel(originalClass);
          
          // Update the class name in the prediction result
          const enhancedResult = {
            ...predictionResult,
            class: translatedClass,
            originalClass: originalClass
          };

          // Send tool end event with enhanced prediction results
          await sendSSEMessage(writer, {
            type: StreamMessageType.ToolEnd,
            tool: "text_classification",
            output: enhancedResult,
          });

          // Generate a response based on the translated prediction
          const confidence = predictionResult.confidence;
          
          // Create a human-readable response with the translated class name
          const responseText = `Your text was classified as "${translatedClass}" with ${(confidence * 100).toFixed(2)}% confidence.`;
          
          // Stream the response token by token (simulating streaming)
          const tokens = responseText.split(/(\s+)/);
          for (const token of tokens) {
            await sendSSEMessage(writer, {
              type: StreamMessageType.Token,
              token: token,
            });
            
            // Small delay to simulate streaming
            await new Promise(resolve => setTimeout(resolve, 10));
          }

          // Send completion message
          await sendSSEMessage(writer, { type: StreamMessageType.Done });
          
          // Store the generated response in Convex
          await convex.mutation(api.messages.send, {
            chatId,
            content: responseText,
          });
          
        } catch (streamError) {
          console.error("Error in prediction process:", streamError);
          await sendSSEMessage(writer, {
            type: StreamMessageType.Error,
            error:
              streamError instanceof Error
                ? streamError.message
                : "Prediction processing failed",
          });
        }
      } catch (error) {
        console.error("Error in stream:", error);
        await sendSSEMessage(writer, {
          type: StreamMessageType.Error,
          error: error instanceof Error ? error.message : "Unknown error",
        });
      } finally {
        try {
          await writer.close();
        } catch (closeError) {
          console.error("Error closing writer:", closeError);
        }
      }
    })();

    return response;
  } catch (error) {
    console.error("Error in chat API:", error);
    return NextResponse.json(
      { error: "Failed to process chat request" } as const,
      { status: 500 }
    );
  }
}
